# RAGæŠ€æœ¯æ£€ç´¢éƒ¨åˆ†è¿›é˜¶ç ”ç©¶ä¸ç§‘ç ”åˆ›æ–°æŒ‡å—

ä½ çš„RAGé€šç”¨é—®ç­”ç³»ç»Ÿé¡¹ç›®å·²ç»æ­å»ºäº†ä¸€ä¸ªå¾ˆå¥½çš„åŸºç¡€æ¡†æ¶ã€‚é’ˆå¯¹æ£€ç´¢éƒ¨åˆ†çš„è¿›ä¸€æ­¥å­¦ä¹ å’Œç§‘ç ”åˆ›æ–°ï¼Œæˆ‘å°†ä¸ºä½ æä¾›ç”±æµ…å…¥æ·±çš„æŒ‡å¯¼è·¯çº¿ã€‚

## ç¬¬ä¸€é˜¶æ®µï¼šæ·±å…¥ç†è§£ç°æœ‰æ£€ç´¢æœºåˆ¶

### 1. åˆ†æå½“å‰æ£€ç´¢ç³»ç»Ÿçš„ç»„æˆ
- **å‘é‡æ£€ç´¢**ï¼šä½ ç›®å‰ä½¿ç”¨çš„æ˜¯FAISS + HuggingFaceåµŒå…¥æ¨¡å‹(distiluse-base-multilingual-cased-v2)
- **æ–‡æœ¬åˆ†å‰²**ï¼šRecursiveCharacterTextSplitterä»¥500å­—ç¬¦ä¸ºå•ä½è¿›è¡Œåˆ†å‰²
- **æ£€ç´¢å‚æ•°**ï¼štop-k=3çš„ç›¸ä¼¼åº¦æ£€ç´¢

### 2. åŸºç¡€å®éªŒä¸æ€§èƒ½è¯„ä¼°
å»ºè®®è¿›è¡Œä»¥ä¸‹å®éªŒæ¥ç†è§£å½“å‰ç³»ç»Ÿçš„è¡¨ç°ï¼š

```python
# æ£€ç´¢ç»“æœåˆ†æå®éªŒ
def analyze_retrieval(query, vector_store, top_k=3):
    """
    æ·±å…¥åˆ†ææ£€ç´¢ç»“æœ
    """
    # è·å–åŸå§‹æ£€ç´¢ç»“æœ
    docs = vector_store.similarity_search(query, k=top_k)
    
    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
    embeddings = vector_store.embedding_function
    query_embedding = embeddings.embed_query(query)
    
    print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
    print(f"ğŸ“Š æ£€ç´¢åˆ°çš„{len(docs)}ä¸ªæ–‡æ¡£:")
    
    for i, doc in enumerate(docs):
        doc_embedding = embeddings.embed_query(doc.page_content)
        similarity = np.dot(query_embedding, doc_embedding)
        print(f"\nğŸ“„ æ–‡æ¡£{i+1} (ç›¸ä¼¼åº¦: {similarity:.4f}):")
        print(f"  æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
        print(f"  å†…å®¹: {doc.page_content[:200]}...")

# ç¤ºä¾‹ä½¿ç”¨
queries = [
    "äº§å“è´¨é‡ç›‘ç£æŠ½æŸ¥çš„è§„å®š",
    "äº§å“ç¼ºé™·çš„æ³•å¾‹å®šä¹‰",
    "è¿åäº§å“è´¨é‡æ³•çš„å¤„ç½šæªæ–½"
]

for query in queries:
    analyze_retrieval(query, vector_store)
```

### 3. æ£€ç´¢è´¨é‡è¯„ä¼°æŒ‡æ ‡
å»ºç«‹ç®€å•çš„è¯„ä¼°ä½“ç³»ï¼š
- å¬å›ç‡(Recall)ï¼šç›¸å…³æ–‡æ¡£è¢«æ£€ç´¢åˆ°çš„æ¯”ä¾‹
- å‡†ç¡®ç‡(Precision)ï¼šæ£€ç´¢ç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
- å¹³å‡å€’æ•°æ’å(MRR)ï¼šç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£æ’åçš„å€’æ•°

```python
# ç®€æ˜“è¯„ä¼°æ¡†æ¶
def evaluate_retrieval(query, relevant_doc_indices, vector_store, top_k=3):
    """
    è¯„ä¼°å•ä¸ªæŸ¥è¯¢çš„æ£€ç´¢æ•ˆæœ
    """
    docs = vector_store.similarity_search(query, k=top_k)
    retrieved_indices = [doc.metadata.get('page', -1) for doc in docs]
    
    # è®¡ç®—æŒ‡æ ‡
    relevant_retrieved = len(set(retrieved_indices) & set(relevant_doc_indices))
    precision = relevant_retrieved / top_k
    recall = relevant_retrieved / len(relevant_doc_indices)
    
    try:
        first_relevant_rank = min([i+1 for i, idx in enumerate(retrieved_indices) 
                                 if idx in relevant_doc_indices])
        mrr = 1 / first_relevant_rank
    except:
        mrr = 0
    
    return {
        'query': query,
        'precision': precision,
        'recall': recall,
        'mrr': mrr
    }

# ç¤ºä¾‹è¯„ä¼°
evaluation_results = []
evaluation_results.append(evaluate_retrieval(
    "äº§å“è´¨é‡ç›‘ç£æŠ½æŸ¥çš„è§„å®š",
    [1, 2, 3],  # å‡è®¾è¿™äº›é¡µç åŒ…å«ç›¸å…³å†…å®¹
    vector_store
))
```

## ç¬¬äºŒé˜¶æ®µï¼šæ£€ç´¢ç»„ä»¶ä¼˜åŒ–ä¸è¿›é˜¶æŠ€æœ¯

### 1. æ–‡æœ¬åˆ†å‰²ä¼˜åŒ–
å½“å‰çš„åˆ†å‰²ç­–ç•¥å¯èƒ½ç ´åæ–‡æ¡£çš„è¯­ä¹‰è¿è´¯æ€§ï¼Œå¯ä»¥å°è¯•ï¼š

```python
# æ”¹è¿›çš„åˆ†å‰²ç­–ç•¥
from langchain.text_splitter import SpacyTextSplitter

def advanced_text_split(docs):
    # ä½¿ç”¨ä¸­æ–‡NLPæ„ŸçŸ¥çš„åˆ†å‰²
    text_splitter = SpacyTextSplitter(
        pipeline="zh_core_web_sm",
        chunk_size=300,
        chunk_overlap=50
    )
    
    # æˆ–è€…å°è¯•è¯­ä¹‰åˆ†å‰²
    from langchain_experimental.text_splitter import SemanticChunker
    from langchain.embeddings import OpenAIEmbeddings
    
    embeddings = OpenAIEmbeddings()
    splitter = SemanticChunker(embeddings, breakpoint_threshold_type="percentile")
    
    return splitter.split_documents(docs)
```

### 2. åµŒå…¥æ¨¡å‹ä¼˜åŒ–
å°è¯•ä¸åŒçš„åµŒå…¥æ¨¡å‹å¹¶æ¯”è¾ƒæ•ˆæœï¼š

```python
# åµŒå…¥æ¨¡å‹æ¯”è¾ƒ
from langchain.embeddings import HuggingFaceBgeEmbeddings

embedding_models = {
    "bge-small": HuggingFaceBgeEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        encode_kwargs={'normalize_embeddings': True}
    ),
    "m3e-base": HuggingFaceEmbeddings(
        model_name="moka-ai/m3e-base"
    ),
    "text2vec": HuggingFaceEmbeddings(
        model_name="GanymedeNil/text2vec-large-chinese"
    )
}

def compare_embeddings(query, docs, embedding_models):
    results = {}
    for name, model in embedding_models.items():
        vector_store = FAISS.from_documents(docs, model)
        retrieved = vector_store.similarity_search(query, k=3)
        results[name] = [doc.page_content[:100] for doc in retrieved]
    
    return results
```

### 3. æ··åˆæ£€ç´¢ç­–ç•¥
ç»“åˆå…³é”®è¯æ£€ç´¢å’Œå‘é‡æ£€ç´¢ï¼š

```python
from rank_bm25 import BM25Okapi
from sklearn.feature_extraction.text import TfidfVectorizer

class HybridRetriever:
    def __init__(self, docs):
        self.docs = [doc.page_content for doc in docs]
        self.bm25 = BM25Okapi([doc.split() for doc in self.docs])
        self.vectorizer = TfidfVectorizer()
        self.tfidf = self.vectorizer.fit_transform(self.docs)
        
    def retrieve(self, query, alpha=0.5, k=5):
        # BM25æ£€ç´¢
        bm25_scores = self.bm25.get_scores(query.split())
        
        # TF-IDFæ£€ç´¢
        query_vec = self.vectorizer.transform([query])
        tfidf_scores = (query_vec * self.tfidf.T).toarray()[0]
        
        # å½’ä¸€åŒ–å¹¶æ··åˆ
        bm25_scores_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())
        tfidf_scores_norm = (tfidf_scores - tfidf_scores.min()) / (tfidf_scores.max() - tfidf_scores.min())
        
        combined_scores = alpha * bm25_scores_norm + (1-alpha) * tfidf_scores_norm
        top_indices = np.argsort(combined_scores)[-k:][::-1]
        
        return [(self.docs[i], combined_scores[i]) for i in top_indices]
```

## ç¬¬ä¸‰é˜¶æ®µï¼šå‰æ²¿ç ”ç©¶ä¸åˆ›æ–°æ–¹å‘

### 1. æŸ¥è¯¢æ‰©å±•ä¸é‡å†™
```python
# æŸ¥è¯¢æ‰©å±•æŠ€æœ¯
from transformers import T5ForConditionalGeneration, T5Tokenizer

class QueryExpander:
    def __init__(self, model_name="castorini/t5-base-canard"):
        self.tokenizer = T5Tokenizer.from_pretrained(model_name)
        self.model = T5ForConditionalGeneration.from_pretrained(model_name)
        
    def expand(self, query, context=None, max_length=32):
        input_text = f"expand: {query}"
        if context:
            input_text += f" context: {context}"
            
        inputs = self.tokenizer(input_text, return_tensors="pt")
        outputs = self.model.generate(
            inputs.input_ids,
            max_length=max_length,
            num_beams=5,
            early_stopping=True
        )
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

# ä½¿ç”¨ç¤ºä¾‹
expander = QueryExpander()
expanded_query = expander.expand("äº§å“è´¨é‡è´£ä»»")
```

### 2. æ£€ç´¢ç»“æœé‡æ’åº
```python
# åŸºäºäº¤å‰ç¼–ç å™¨çš„é‡æ’åº
from sentence_transformers import CrossEncoder

class Reranker:
    def __init__(self, model_name="BAAI/bge-reranker-base"):
        self.model = CrossEncoder(model_name)
    
    def rerank(self, query, documents, top_k=3):
        pairs = [(query, doc) for doc in documents]
        scores = self.model.predict(pairs)
        ranked_indices = np.argsort(scores)[::-1][:top_k]
        return [documents[i] for i in ranked_indices], [scores[i] for i in ranked_indices]

# ä½¿ç”¨ç¤ºä¾‹
reranker = Reranker()
retrieved_docs = vector_store.similarity_search(query, k=10)
reranked_docs, scores = reranker.rerank(query, [doc.page_content for doc in retrieved_docs])
```

### 3. åŠ¨æ€æ£€ç´¢ä¼˜åŒ–
```python
# è‡ªé€‚åº”æ£€ç´¢æ¡†æ¶
class AdaptiveRetriever:
    def __init__(self, vector_store, llm):
        self.vector_store = vector_store
        self.llm = llm
        self.query_history = []
        
    def generate_search_terms(self, query):
        prompt = f"""
        æ ¹æ®ä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆ3ä¸ªæœ€ç›¸å…³çš„æœç´¢å…³é”®è¯ï¼š
        é—®é¢˜ï¼š{query}
        å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼š
        """
        keywords = self.llm.invoke(prompt).strip().split(",")
        return [k.strip() for k in keywords if k.strip()]
    
    def retrieve(self, query, k=3):
        # ç”Ÿæˆå¤šä¸ªæœç´¢è¯
        search_terms = self.generate_search_terms(query)
        self.query_history.append((query, search_terms))
        
        # å¤šæœ¯è¯­æ£€ç´¢
        all_docs = []
        for term in search_terms:
            docs = self.vector_store.similarity_search(term, k=k)
            all_docs.extend(docs)
            
        # å»é‡å¹¶æ’åº
        unique_docs = {doc.page_content: doc for doc in all_docs}.values()
        return sorted(unique_docs, key=lambda x: -self._calculate_relevance(x.page_content, query))[:k]
    
    def _calculate_relevance(self, doc, query):
        # å®ç°è‡ªå®šä¹‰ç›¸å…³æ€§è®¡ç®—
        return len(set(doc.split()) & set(query.split())) / len(set(query.split()))
```

## ç¬¬å››é˜¶æ®µï¼šç§‘ç ”åˆ›æ–°æ–¹å‘å»ºè®®

### 1. é¢†åŸŸè‡ªé€‚åº”æ£€ç´¢
- ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•ä½¿é€šç”¨åµŒå…¥æ¨¡å‹é€‚åº”ç‰¹å®šæ³•å¾‹é¢†åŸŸ
- åˆ›æ–°ç‚¹ï¼š
  - æ³•å¾‹é¢†åŸŸç‰¹å®šçš„åµŒå…¥æ¨¡å‹å¾®è°ƒ
  - ç»“åˆæ³•å¾‹çŸ¥è¯†å›¾è°±å¢å¼ºæ£€ç´¢

### 2. å¤šæ¨¡æ€æ³•å¾‹æ£€ç´¢
- ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•å¤„ç†æ³•å¾‹æ–‡æœ¬ä¸­çš„è¡¨æ ¼ã€å›¾è¡¨ç­‰å¤šæ¨¡æ€ä¿¡æ¯
- åˆ›æ–°ç‚¹ï¼š
  - å¼€å‘æ³•å¾‹æ–‡æ¡£ä¸“ç”¨çš„å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹
  - ç ”ç©¶æ–‡æœ¬ä¸æ³•å¾‹å›¾è¡¨çš„å…³ç³»å»ºæ¨¡

### 3. æ£€ç´¢å¯è§£é‡Šæ€§ç ”ç©¶
- ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•è®©RAGç³»ç»Ÿçš„æ£€ç´¢è¿‡ç¨‹æ›´é€æ˜
- åˆ›æ–°ç‚¹ï¼š
  - å¼€å‘æ£€ç´¢è·¯å¾„å¯è§†åŒ–å·¥å…·
  - è®¾è®¡åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ£€ç´¢è§£é‡Šæ¨¡å‹

### 4. åŠ¨æ€æ£€ç´¢ç­–ç•¥
- ç ”ç©¶é—®é¢˜ï¼šå¦‚ä½•æ ¹æ®å¯¹è¯å†å²ä¼˜åŒ–å½“å‰æ£€ç´¢
- åˆ›æ–°ç‚¹ï¼š
  - åŸºäºå¼ºåŒ–å­¦ä¹ çš„åŠ¨æ€æ£€ç´¢ç­–ç•¥
  - å¯¹è¯æ„ŸçŸ¥çš„æŸ¥è¯¢é‡å†™æœºåˆ¶

## å®æ–½å»ºè®®

1. **å»ºç«‹åŸºå‡†æµ‹è¯•é›†**ï¼šä»ã€Šä¸­å›½äº§å“è´¨é‡æ³•ã€‹ä¸­æ„å»ºåŒ…å«é—®é¢˜-æ ‡å‡†ç­”æ¡ˆ-ç›¸å…³æ¡æ¬¾çš„æµ‹è¯•é›†

2. **å®éªŒè®°å½•**ï¼šä½¿ç”¨MLflowæˆ–Weights & Biasesè®°å½•ä¸åŒæ£€ç´¢é…ç½®çš„æ•ˆæœ

3. **é€æ­¥åˆ›æ–°**ï¼šä»ä¼˜åŒ–ç°æœ‰ç»„ä»¶å¼€å§‹ï¼Œé€æ­¥å¼•å…¥åˆ›æ–°æ–¹æ³•

4. **å­¦æœ¯è°ƒç ”**ï¼šå®šæœŸé˜…è¯»ACLã€EMNLPç­‰é¡¶ä¼šä¸­å…³äºRAGçš„æœ€æ–°ç ”ç©¶

ä½ çš„é¡¹ç›®å·²ç»å…·å¤‡äº†å¾ˆå¥½çš„åŸºç¡€ï¼Œé€šè¿‡ç³»ç»Ÿæ€§åœ°æ¢ç´¢è¿™äº›è¿›é˜¶æ–¹å‘ï¼Œå¯ä»¥äº§ç”Ÿæœ‰ä»·å€¼çš„ç§‘ç ”æˆæœã€‚å»ºè®®å…ˆä»ç¬¬äºŒé˜¶æ®µçš„ä¼˜åŒ–å¼€å§‹ï¼Œå»ºç«‹é‡åŒ–è¯„ä¼°æŒ‡æ ‡åå†æ¨è¿›åˆ°åˆ›æ–°ç ”ç©¶ã€‚