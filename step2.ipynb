{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 检索组件优化与进阶技术\n",
    "---"
   ],
   "id": "ed82b365035cbbf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. 环境准备\n",
    "- 安装依赖包\n",
    "- 设置镜像源"
   ],
   "id": "72beb674f6ad25df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 设置国内镜像源（加速下载）\n",
    "import os\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "# 安装依赖包（取消注释运行）\n",
    "# !pip install langchain faiss-cpu huggingface-hub dashscope PyPDF2"
   ],
   "id": "2d39376021f86bd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. 文档处理\n",
    "- 加载PDF文件\n",
    "- 检查加载结果"
   ],
   "id": "50f4f4f4ef79ab61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_documents(pdf_paths):\n",
    "    \"\"\"加载pdf文档并返回LangChain Document对象列表\"\"\"\n",
    "    all_docs = []\n",
    "    for path in pdf_paths:\n",
    "        try:\n",
    "            loader = PyPDFLoader(path)\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "            print(f\"成功加载：{path}（共{len(docs)}页）\")\n",
    "        except Exception as e:\n",
    "            print(f\"加载失败{path}：{str(e)}\")\n",
    "    return all_docs\n",
    "\n",
    "# 示例使用\n",
    "pdf_paths = [\"pdf_China/中国产品质量法.pdf\"]\n",
    "processed_documents = load_documents(pdf_paths) # 原始文件\n",
    "print(processed_documents)"
   ],
   "id": "3395d9979278c339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. 文本分割与向量化\n",
    "\n",
    "- 中文文本分割"
   ],
   "id": "33662e0603888769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(docs):\n",
    "    \"\"\"执行文本分割\"\"\"\n",
    "    if not docs:\n",
    "        raise ValueError(\"输入文档列表为空\")\n",
    "\n",
    "    # 中文优化分割器\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,     # 每个文本块500字符\n",
    "        chunk_overlap=100,  # 块间重叠100字符\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\"] # 中文分隔符\n",
    "    )\n",
    "\n",
    "    # 执行分割\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"分割为{len(split_docs)}个文本块\")\n",
    "\n",
    "    # 查看前2个分割样例\n",
    "    for i, doc in enumerate(split_docs[:2]):\n",
    "        print(f\"\\n块{i+1}:\\n{doc.page_content[:200]}...\")\n",
    "\n",
    "    return split_docs\n",
    "\n",
    "# 示例使用\n",
    "split_decs = split_documents(processed_documents)"
   ],
   "id": "2c6e4df9f1971283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 文本分割优化",
   "id": "4dce201b23728fed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # 改进的分割策略\n",
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "#\n",
    "# def advanced_text_split(docs):\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "#     splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "#\n",
    "# # 示例使用\n",
    "# split_decs2 = advanced_text_split(processed_documents)\n",
    "# print(split_decs2)"
   ],
   "id": "b6b5e069fed7910d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 构建向量数据库",
   "id": "76896ea07d19867c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def create_vector_store(split_docs):\n",
    "    \"\"\"创建并返回向量数据库\"\"\"\n",
    "    if not split_docs:\n",
    "        raise ValueError(\"无法用空文档创建向量库\")\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"distiluse-base-multilingual-cased-v2\"\n",
    "    )\n",
    "\n",
    "    vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "    print(f\"向量库已构建（包含{vector_store.index.ntotal}）个向量\")\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "# 示例使用\n",
    "vector_store = create_vector_store(split_decs)"
   ],
   "id": "5f912b0710bf7a81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 检查向量数据库中的基本信息",
   "id": "ab59e89afa71fcd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"向量数据库类型：{type(vector_store)}\")\n",
    "\n",
    "print(f\"向量数据库中文档数量：{vector_store.index.ntotal}\")\n",
    "\n",
    "print(f\"向量维度：{vector_store.index.d}\")"
   ],
   "id": "bca24596fe498df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. 分析当前检索系统的组成\n",
    "\n",
    "- **向量检索**：目前使用的是FAISS + HuggingFace嵌入模型(distiluse-base-multilingual-cased-v2)\n",
    "- **文本分割**：RecursiveCharacterTextSplitter以500字符为单位进行分割\n",
    "- **检索参数**：top-k=3的相似度检索"
   ],
   "id": "ffa8c3af72813e8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. 基础实验与性能评估",
   "id": "d6e8cc2fffd75b10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# 检索分析实验\n",
    "def analyze_retrieval(query, vector_store, top_k=3):\n",
    "    \"\"\"深入分析实验结果\"\"\"\n",
    "    # 获取原始检索结果（这句是集成的功能，下面的才是将其分开的细节）\n",
    "    docs = vector_store.similarity_search(query, k=top_k)\n",
    "\n",
    "    # 计算相似度分数\n",
    "    embeddings = vector_store.embedding_function # 返回嵌入模型对象（vector_store没有其他用）\n",
    "    query_embedding = embeddings.embed_query(query) # 调用嵌入模型对问题编码\n",
    "\n",
    "    print(query_embedding) # 输出问题的向量\n",
    "\n",
    "    print(f\"\\n查询：'{query}'\")\n",
    "    print(f\"检索到的{len(docs)}个文档：\")\n",
    "\n",
    "    for i, doc in enumerate(docs[:top_k]):\n",
    "        doc_embedding = embeddings.embed_query(doc.page_content) # 调用嵌入模型对检索到的文本编码\n",
    "        similarity = np.dot(query_embedding, doc_embedding) # 计算查询向量和文档向量之间的余弦相似度\n",
    "        print(f\"\\n📄 文档{i+1} (相似度: {similarity:.4f}):\")\n",
    "        print(f\"  来源: {doc.metadata.get('source', '未知')}\")\n",
    "        print(f\"  内容: {doc.page_content[:50]}...\")\n",
    "\n",
    "# 示例使用\n",
    "queries = [\n",
    "    \"产品质量监督抽查的规定\",\n",
    "    \"产品缺陷的法律定义\",\n",
    "    \"违反产品质量法的处罚措施\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    analyze_retrieval(query, vector_store)\n"
   ],
   "id": "6b7ae413c75a38e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. 检索质量评估指标\n",
    "\n",
    "**建立简单的评估体系**：\n",
    "- 召回率(Recall)：相关文档被检索到的比例\n",
    "- 准确率(Precision)：检索结果中相关文档的比例\n",
    "- 平均倒数排名(MRR)：第一个相关文档排名的倒数"
   ],
   "id": "a2f8258dda728cf0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 简易评估框架\n",
    "def evaluate_retrieval(query, relevant_doc_indices, vector_store, top_k=3):\n",
    "    \"\"\"评估单个查询的检索结果\"\"\"\n",
    "    # 检索到的文档信息\n",
    "    docs = vector_store.similarity_search(query, k=top_k)\n",
    "    # 提取检索到的页码\n",
    "    retrieved_indices = [doc.metadata.get('page', -1) for doc in docs]\n",
    "\n",
    "    print(\"docs类型：\", type(docs))\n",
    "    # print(docs)\n",
    "\n",
    "    print(\"retrieved_indices类型：\", type(retrieved_indices))\n",
    "    print(\"检索到的文档页码：\", retrieved_indices)\n",
    "\n",
    "    # 计算指标\n",
    "    # 检索到的又相关的 = 检索到的 & 相关的\n",
    "    relevant_retrieved = len(set(retrieved_indices) & set(relevant_doc_indices))\n",
    "\n",
    "    # 准确率 = 检索到的相关文档数/检索到的文档总数\n",
    "    precision = relevant_retrieved / top_k\n",
    "\n",
    "    # 召回率 = 检索到的相关文档数 / 所有相关文档数\n",
    "    recall = relevant_retrieved / len(relevant_doc_indices)\n",
    "\n",
    "    # 平均倒数排名（在检索到的文档中，第一个相关文档排名的倒数）（省略了多次查询求平均的步骤）\n",
    "    try:\n",
    "        first_relevant_doc = min([i+1 for i, idx in enumerate(retrieved_indices)\n",
    "                                  if idx in relevant_doc_indices])\n",
    "        mrr = 1 / first_relevant_doc\n",
    "    except :\n",
    "        mrr = 0\n",
    "\n",
    "    # 返回一个字典\n",
    "    return {\n",
    "        'query': query,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'mrr': mrr\n",
    "    }\n",
    "\n",
    "# 示例评估\n",
    "evaluation_results = []\n",
    "evaluation_results.append(evaluate_retrieval(\n",
    "    \"产品质量监督抽查的规定\",\n",
    "    [1, 2, 3], # 假设这些页码包含相关内容\n",
    "    vector_store\n",
    "))\n",
    "print(evaluation_results)"
   ],
   "id": "36370bf0675e477",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
